import requests
from bs4 import BeautifulSoup
import os
import json

# 1. ëŒ€ìƒ ëŒ€í•™ ì„¤ì • (ëŒ€í•™ëª…, ê²Œì‹œíŒ URL, ë°ì´í„° ì¶”ì¶œìš© CSS ì„ íƒì)
UNIVERSITIES = [
    {
        "name": "ê°•ë¦‰ì›ì£¼ëŒ€",
        "url": "https://www.gwnu.ac.kr/kr/7924/subview.do",
        "selector": "tr:not(.notice) td.td-subject a", # ê³µì§€(notice) ì œì™¸í•œ ì¼ë°˜ê¸€
        "base_url": "https://www.gwnu.ac.kr"
    },
    {
        "name": "ê°€í†¨ë¦­ê´€ë™ëŒ€",
        "url": "https://www.cku.ac.kr/cku_kr/5787/subview.do?enc=Zm5jdDF8QEB8JTJGYmJzJTJGY2t1X2tyJTJGMTIwMiUyRmFydGNsTGlzdC5kbyUzRmJic0NsU2VxJTNEMTU4NCUyNmJic09wZW5XcmRTZXElM0QlMjZpc1ZpZXdNaW5lJTNEZmFsc2UlMjZzcmNoQ29sdW1uJTNEc2olMjZzcmNoV3JkJTNEJTI2",
        "selector": "tr:not(.notice) td.td-subject a",
        "base_url": "https://www.cku.ac.kr"
    },
    {
        "name": "ê°•ë¦‰ì˜ë™ëŒ€",
        "url": "https://www.gyu.ac.kr/gyu/selectBbsNttList.do?bbsNo=210&key=387",
        "selector": "tr:not(.notice) td.td-subject a",
        "base_url": "https://www.gyc.ac.kr"
    }
]

DB_FILE = "last_posts.json"

def load_last_posts():
    if os.path.exists(DB_FILE):
        with open(DB_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_last_posts(data):
    with open(DB_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def send_telegram(message):
    # ë³´ì•ˆì„ ìœ„í•´ í† í°ê³¼ IDëŠ” GitHub Secrets ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    token = os.environ.get("TELEGRAM_TOKEN")
    chat_id = os.environ.get("TELEGRAM_CHAT_ID")
    if not token or not chat_id:
        print("í† í° ë˜ëŠ” ì±„íŒ… IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    params = {"chat_id": chat_id, "text": message}
    requests.get(url, params=params)

def crawl():
    last_posts = load_last_posts()
    new_data = last_posts.copy()

    for univ in UNIVERSITIES:
        try:
            response = requests.get(univ["url"], timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ìµœì‹ ê¸€ í•˜ë‚˜ ì¶”ì¶œ
            post_element = soup.select_one(univ["selector"])
            if not post_element:
                continue

            title = post_element.get_text(strip=True)
            link = univ["base_url"] + post_element['href']
            
            # ì´ì „ ì €ì¥ëœ ì œëª©ê³¼ ë¹„êµ
            if last_posts.get(univ["name"]) != title:
                print(f"[{univ['name']}] ìƒˆë¡œìš´ ê³µê³  ë°œê²¬!")
                message = f"ğŸ“¢ [{univ['name']}] ìƒˆ ì±„ìš©ê³µê³ \nì œëª©: {title}\në§í¬: {link}"
                send_telegram(message)
                new_data[univ["name"]] = title
            else:
                print(f"[{univ['name']}] ìƒˆë¡œìš´ ê³µê³  ì—†ìŒ")
                
        except Exception as e:
            print(f"{univ['name']} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    save_last_posts(new_data)

if __name__ == "__main__":
    crawl()
